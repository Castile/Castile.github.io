<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks-ICCV2019 | Castile</title><meta name="description" content="Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks-ICCV2019"><meta name="keywords" content="papers,VOS"><meta name="author" content="朱宏梁"><meta name="copyright" content="朱宏梁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks-ICCV2019"><meta name="twitter:description" content="Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks-ICCV2019"><meta name="twitter:image" content="https://Castile.github.io/img/cover/agnn.png"><meta property="og:type" content="article"><meta property="og:title" content="Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks-ICCV2019"><meta property="og:url" content="https://castile.github.io/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/"><meta property="og:site_name" content="Castile"><meta property="og:description" content="Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks-ICCV2019"><meta property="og:image" content="https://Castile.github.io/img/cover/agnn.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://castile.github.io/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/"><link rel="next" title="Java之IO流" href="https://castile.github.io/2020/02/17/Java%E4%B9%8BIO%E6%B5%81/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: {"text":"富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善","fontSize":"15px"},
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Castile" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Castile</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> 阅读</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">32</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">31</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">5</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> 阅读</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Zero-Shot-Video-Object-Segmentation-via-Attentive-Graph-Neural-Networks（ICCV2019）"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks（ICCV2019）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#通过图注意力神经网络的Zero-Shot视频目标分割"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">通过图注意力神经网络的Zero-Shot视频目标分割</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#代码：-https-github-com-Castile-AGNNForVOS"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">代码：  https:&#x2F;&#x2F;github.com&#x2F;Castile&#x2F;AGNNForVOS</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#一、-概述"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">一、 概述</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#二、-目前的方法"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">二、 目前的方法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-Zero-Shot-solution"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">1. Zero-Shot solution</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-基于FCN的："><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">2. 基于FCN的：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#3-双流网络来融合外观信息和运动信息"><span class="toc_mobile_items-number">4.3.</span> <span class="toc_mobile_items-text">3. 双流网络来融合外观信息和运动信息</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#三、-提出的方法-ZVOS"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">三、 提出的方法-ZVOS</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#四、-图神经网络GNN"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">四、 图神经网络GNN</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-GNN-的-Survey-papers"><span class="toc_mobile_items-number">6.1.</span> <span class="toc_mobile_items-text">1. GNN 的 Survey papers</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-GNN-的表示"><span class="toc_mobile_items-number">6.2.</span> <span class="toc_mobile_items-text">2. GNN 的表示</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#五、-AGNN"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">五、 AGNN</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-AGNN"><span class="toc_mobile_items-number">7.1.</span> <span class="toc_mobile_items-text">1. AGNN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-符号表示"><span class="toc_mobile_items-number">7.2.</span> <span class="toc_mobile_items-text">2. 符号表示</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#FCN-Based-Node-Embedding"><span class="toc_mobile_items-number">7.3.</span> <span class="toc_mobile_items-text">FCN-Based Node Embedding</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Intra-Attention-Based-Loop-Edge-Embedding"><span class="toc_mobile_items-number">7.4.</span> <span class="toc_mobile_items-text">Intra-Attention Based Loop-Edge Embedding</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Inter-Attention-Based-Line-Edge-Embedding"><span class="toc_mobile_items-number">7.5.</span> <span class="toc_mobile_items-text">Inter-Attention Based Line-Edge Embedding</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Gated-Message-Aggregation"><span class="toc_mobile_items-number">7.6.</span> <span class="toc_mobile_items-text">Gated Message Aggregation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#ConvGRU-based-Node-State-Update-更新节点状态"><span class="toc_mobile_items-number">7.7.</span> <span class="toc_mobile_items-text">ConvGRU based Node-State Update[更新节点状态]</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Readout-Function-读出函数-预测"><span class="toc_mobile_items-number">7.8.</span> <span class="toc_mobile_items-text">Readout Function  [读出函数-预测]</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#六、-网络的具体信息"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">六、 网络的具体信息</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#训练：损失函数"><span class="toc_mobile_items-number">8.1.</span> <span class="toc_mobile_items-text">训练：损失函数</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#七、-结果"><span class="toc_mobile_items-number">9.</span> <span class="toc_mobile_items-text">七、 结果</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Zero-Shot-Video-Object-Segmentation-via-Attentive-Graph-Neural-Networks（ICCV2019）"><span class="toc-number">1.</span> <span class="toc-text">Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks（ICCV2019）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#通过图注意力神经网络的Zero-Shot视频目标分割"><span class="toc-number">2.</span> <span class="toc-text">通过图注意力神经网络的Zero-Shot视频目标分割</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#代码：-https-github-com-Castile-AGNNForVOS"><span class="toc-number">2.1.</span> <span class="toc-text">代码：  https:&#x2F;&#x2F;github.com&#x2F;Castile&#x2F;AGNNForVOS</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#一、-概述"><span class="toc-number">3.</span> <span class="toc-text">一、 概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、-目前的方法"><span class="toc-number">4.</span> <span class="toc-text">二、 目前的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Zero-Shot-solution"><span class="toc-number">4.1.</span> <span class="toc-text">1. Zero-Shot solution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-基于FCN的："><span class="toc-number">4.2.</span> <span class="toc-text">2. 基于FCN的：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-双流网络来融合外观信息和运动信息"><span class="toc-number">4.3.</span> <span class="toc-text">3. 双流网络来融合外观信息和运动信息</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、-提出的方法-ZVOS"><span class="toc-number">5.</span> <span class="toc-text">三、 提出的方法-ZVOS</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#四、-图神经网络GNN"><span class="toc-number">6.</span> <span class="toc-text">四、 图神经网络GNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-GNN-的-Survey-papers"><span class="toc-number">6.1.</span> <span class="toc-text">1. GNN 的 Survey papers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-GNN-的表示"><span class="toc-number">6.2.</span> <span class="toc-text">2. GNN 的表示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#五、-AGNN"><span class="toc-number">7.</span> <span class="toc-text">五、 AGNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-AGNN"><span class="toc-number">7.1.</span> <span class="toc-text">1. AGNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-符号表示"><span class="toc-number">7.2.</span> <span class="toc-text">2. 符号表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FCN-Based-Node-Embedding"><span class="toc-number">7.3.</span> <span class="toc-text">FCN-Based Node Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Intra-Attention-Based-Loop-Edge-Embedding"><span class="toc-number">7.4.</span> <span class="toc-text">Intra-Attention Based Loop-Edge Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inter-Attention-Based-Line-Edge-Embedding"><span class="toc-number">7.5.</span> <span class="toc-text">Inter-Attention Based Line-Edge Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gated-Message-Aggregation"><span class="toc-number">7.6.</span> <span class="toc-text">Gated Message Aggregation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ConvGRU-based-Node-State-Update-更新节点状态"><span class="toc-number">7.7.</span> <span class="toc-text">ConvGRU based Node-State Update[更新节点状态]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Readout-Function-读出函数-预测"><span class="toc-number">7.8.</span> <span class="toc-text">Readout Function  [读出函数-预测]</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#六、-网络的具体信息"><span class="toc-number">8.</span> <span class="toc-text">六、 网络的具体信息</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#训练：损失函数"><span class="toc-number">8.1.</span> <span class="toc-text">训练：损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#七、-结果"><span class="toc-number">9.</span> <span class="toc-text">七、 结果</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(/img/cover/agnn.png)"><div id="post-info"><div id="post-title"><div class="posttitle">Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks-ICCV2019</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-02-19<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-02-19</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.3k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 12 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="Zero-Shot-Video-Object-Segmentation-via-Attentive-Graph-Neural-Networks（ICCV2019）"><a href="#Zero-Shot-Video-Object-Segmentation-via-Attentive-Graph-Neural-Networks（ICCV2019）" class="headerlink" title="Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks（ICCV2019）"></a>Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks（ICCV2019）</h1><p>  Wenguan Wang1∗,  Xiankai Lu1∗,  Jianbing Shen1y,  David Crandall2,  Ling Shao1<br>       Inception Institute of Artificial Intelligence,   UAE  Indiana University, USA  </p>
<h1 id="通过图注意力神经网络的Zero-Shot视频目标分割"><a href="#通过图注意力神经网络的Zero-Shot视频目标分割" class="headerlink" title="通过图注意力神经网络的Zero-Shot视频目标分割"></a>通过图注意力神经网络的Zero-Shot视频目标分割</h1><h2 id="代码：-https-github-com-Castile-AGNNForVOS"><a href="#代码：-https-github-com-Castile-AGNNForVOS" class="headerlink" title="代码：  https://github.com/Castile/AGNNForVOS"></a>代码：  <a href="https://github.com/Castile/AGNNForVOS" target="_blank" rel="noopener">https://github.com/Castile/AGNNForVOS</a></h2><h1 id="一、-概述"><a href="#一、-概述" class="headerlink" title="一、 概述"></a>一、 概述</h1><p>​        提出了一种基于图注意力神经网络的用于Zero-Shot的视频目标分割算法。 AGNN将此任务重新定义为在图上进行迭代信息融合的过程 。 具体来说，AGNN构建一个全连通图，有效地将帧表示为节点，任意帧对之间的关系表示为边。 潜在的两两关系由一个可微分的注意机制来描述。通过参数化消息传递，AGNN能够有效地捕获和挖掘视频帧之间更丰富、更高阶的关系，从而更全面地理解视频内容，更准确地估计前景。大量的实验验证了AGNN能够学习视频帧或相关图像之间潜在的语义或者外观关系，并找出共同的目标。【基于全局的视角】  </p>
<hr>
<h1 id="二、-目前的方法"><a href="#二、-目前的方法" class="headerlink" title="二、 目前的方法"></a>二、 目前的方法</h1><p>​        基于深度学习的方法需要大量的训练数据，利用<strong>双流网络</strong>来结合局部信息和外观信息，使用光流来对连续两帧进行运动的建模，使用RNN对时序建模。但是它们普遍存在两个<strong>局限性</strong>。首先，它们主要关注连续帧之间的局部成对或顺序关系，而忽略了帧之间普遍存在的高阶关系(因为来自同一视频的帧通常是相关的)。其次，由于他们没有充分利用丰富的关系，他们不能完全捕捉视频内容，因此对前景的估计效果很差。从另一个角度来看，由于视频对象通常存在底层对象遮挡、尺度变化大、外观变化大，仅考虑视频中连续关系或局部成对关系时，很难正确推断前景。</p>
<p>Zero-Shot参考：《  <a href="https://arxiv.org/pdf/1903.05612" target="_blank" rel="noopener">Rvos: End to-end recurrent network for video object segmentation</a>  》</p>
<p> 采用双流网络结合局部运动和外观信息，采用RNN逐帧建模 ：</p>
<ol>
<li>Segflow: Joint learning for video object segmentation and optical flow. In ICCV, 2017  </li>
<li>Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos. In CVPR,2017.</li>
<li>Learning video object segmentation with visual memory. In ICCV, 2017.  </li>
<li>Unsupervised video object segmentation with motion-based bilateral networks. In ECCV, 2018.  </li>
<li>Pyramid dilated deeper convlstm for video salient object detection. In ECCV, 2018  </li>
</ol>
<hr>
<h2 id="1-Zero-Shot-solution"><a href="#1-Zero-Shot-solution" class="headerlink" title="1. Zero-Shot solution"></a>1. Zero-Shot solution</h2><ol>
<li><p>Learning to segment moving objects in videos. In CVPR, 2015 ： 设计了一种基于多层感知的运动目标检测系统</p>
</li>
<li><p><strong>Fusionseg</strong>: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos. In CVPR,2017. ： </p>
</li>
<li><p><strong>Learning video object segmentation with visual memory. In <em>ICCV</em>, 2017.</strong> </p>
</li>
<li><p>Segflow: Joint learning for video object segmentation and optical flow. In <em>ICCV</em>, 2017</p>
</li>
<li><p><strong>Instance embedding transfer to unsupervised video object segmentation. In <em>CVPR</em>, 2018.</strong> ： 整合深度学习的实例嵌入和运动显著性来提高性能。</p>
</li>
<li><p><strong>Unsupervised video object segmentation with motion-based bilateral networks. In <em>ECCV</em>, 2018.</strong></p>
</li>
<li><p>Flow guided recurrent neural encoder for video salient object detection. In <em>CVPR</em>, 2018.</p>
</li>
<li><p><strong>See more, know more: Unsupervised video object segmentation with co-attention Siamese networks. In <em>CVPR</em>, 2019.</strong></p>
</li>
<li><p>The graph neural network model. <em>IEEE TNNLS</em>, 20(1):61–80, 2009</p>
</li>
<li><p>Neural message passing for quantum chemistry. In <em>ICML</em>, 2017</p>
</li>
</ol>
<hr>
<h2 id="2-基于FCN的："><a href="#2-基于FCN的：" class="headerlink" title="2. 基于FCN的："></a>2. 基于FCN的：</h2><ol>
<li><p>Triply supervised decoder networks for joint detection and segmentation. In <em>CVPR</em>, 2019</p>
</li>
<li><p>Fully convolutional networks for semantic segmentation. In <em>CVPR</em>, 2015.</p>
</li>
<li><p>Ranet: Ranking attention network for fast video object segmentation. In <em>ICCV</em>, 2019.</p>
</li>
</ol>
<hr>
<h2 id="3-双流网络来融合外观信息和运动信息"><a href="#3-双流网络来融合外观信息和运动信息" class="headerlink" title="3. 双流网络来融合外观信息和运动信息"></a>3. 双流网络来融合外观信息和运动信息</h2><ol>
<li><p>Flow guided recurrent neural encoder for video salient object detection. In <em>CVPR</em>, 2018.</p>
</li>
<li><p><strong>Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos. In <em>CVPR</em>, 2017.</strong></p>
</li>
<li><p>Segflow: Joint learning for video object segmentation and optical flow. In <em>ICCV</em>, 2017</p>
</li>
</ol>
<hr>
<h1 id="三、-提出的方法-ZVOS"><a href="#三、-提出的方法-ZVOS" class="headerlink" title="三、 提出的方法-ZVOS"></a>三、 提出的方法-ZVOS</h1><p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582099550176.png" data-fancybox="group" data-caption="1582099550176" class="fancybox"><img alt="1582099550176" style="zoom:100%;" title="1582099550176" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582099550176.png" class="lazyload"></a></p>
<p>​        提出了一种注意力图神经网络(<code>AGNN</code>)来解决<code>Zero-Shot</code>视频目标分割(ZVOS)问题，将<code>ZVOS</code>重新定义为一种端到端的、基于消息传递的图信息融合过程(如上图b所示)。具体地，构造了一个全连通图，其中视频帧被表示为节点，两帧之间的两两关系被描述为对应节点之间的边。两帧之间的关联被一个注意力机制有效地捕获，这避免了耗时的光流估计。</p>
<hr>
<h1 id="四、-图神经网络GNN"><a href="#四、-图神经网络GNN" class="headerlink" title="四、 图神经网络GNN"></a>四、 图神经网络GNN</h1><p>​        GNN最初是在《 A new model for learning in graph domains. In IJCNN, 2005.》中提出的，并在《         The graph neural network model. IEEE TNNLS, 20(1):61–80, 2009.  》中进一步发展，以处理结构化数据之间的底层关系。</p>
<p>​        在《The graph neural network model》中，使用RNN对每个节点的状态进行建模，通过传递相邻节点的参数化消息来挖掘节点之间的底层关联。近年来，GNNs已成功应用于分子生物学、计算机视觉、机器学习、自然语言处理等诸多领域。GNNs的另一个流行趋势是将卷积体系结构泛化到任意图形结构数据上，即图卷积神经网络(graph convolution neural network, GCNN)</p>
<p>​        提出的AGNN属于前一类;它是一个基于GNN的消息传递，其中所有的节点、边和消息传递函数都由神经网络参数化。它与图上的挖掘关系的一般思想相同，但是有显著的差异。</p>
<p>​        首先，我们的AGNN在空间信息的保留方面是独特的，这与传统的全连通是不同的，而且GNNs对于逐像素预测任务至关重要。其次，为了有效地捕获两个图像帧之间的关系，我们引入了一个可微注意力机制，该机制处理相关信息并产生进一步的鉴别边缘特征。</p>
<h2 id="1-GNN-的-Survey-papers"><a href="#1-GNN-的-Survey-papers" class="headerlink" title="1. GNN 的 Survey papers"></a>1. GNN 的 Survey papers</h2><ol>
<li><p><strong>Graph Neural Networks: A Review of Methods and Applications.</strong> arxiv 2018. <a href="https://arxiv.org/pdf/1812.08434.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Maosong Sun.</em></p>
</li>
<li><p><strong>A Comprehensive Survey on Graph Neural Networks.</strong> arxiv 2019. <a href="https://arxiv.org/pdf/1901.00596.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S. Yu.</em></p>
</li>
<li><p><strong>Deep Learning on Graphs: A Survey.</strong> arxiv 2018. <a href="https://arxiv.org/pdf/1812.04202.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Ziwei Zhang, Peng Cui, Wenwu Zhu.</em></p>
</li>
<li><p><strong>Relational Inductive Biases, Deep Learning, and Graph Networks.</strong> arxiv 2018. <a href="https://arxiv.org/pdf/1806.01261.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others.</em></p>
</li>
<li><p><strong>Geometric Deep Learning: Going beyond Euclidean data.</strong> IEEE SPM 2017. <a href="https://arxiv.org/pdf/1611.08097.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre.</em></p>
</li>
<li><p><strong>Computational Capabilities of Graph Neural Networks.</strong> IEEE TNN 2009. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4703190" target="_blank" rel="noopener">paper</a></p>
<p><em>Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele.</em></p>
</li>
<li><p><strong>Neural Message Passing for Quantum Chemistry.</strong> ICML 2017. <a href="https://arxiv.org/pdf/1704.01212.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E.</em></p>
</li>
<li><p><strong>Non-local Neural Networks.</strong> CVPR 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming.</em></p>
</li>
<li><p><strong>The Graph Neural Network Model.</strong> IEEE TNN 2009. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4700287" target="_blank" rel="noopener">paper</a></p>
<p><em>Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele.</em></p>
</li>
</ol>
<h2 id="2-GNN-的表示"><a href="#2-GNN-的表示" class="headerlink" title="2. GNN 的表示"></a>2. GNN 的表示</h2><p>参考：</p>
<ol>
<li><a href="https://blog.csdn.net/u011748542/article/details/86289511" target="_blank" rel="noopener">图神经网络模型 The Graph Neural Network Model</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/76290138" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76290138</a> </li>
</ol>
<h1 id="五、-AGNN"><a href="#五、-AGNN" class="headerlink" title="五、 AGNN"></a>五、 AGNN</h1><h2 id="1-AGNN"><a href="#1-AGNN" class="headerlink" title="1. AGNN"></a>1. AGNN</h2><blockquote>
<p>AGNN ： </p>
<p>1) 提供统一的、端到端可训练的、基于图模型的ZVOS解决方案;</p>
<p>2) 通过在图上迭代传播和融合消息，有效挖掘视频内部丰富多样的高阶关系;</p>
<p>3) 利用可微注意机制捕获帧对之间的相关信息。</p>
</blockquote>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582102330688.png" data-fancybox="group" data-caption="1582102330688" class="fancybox"><img alt="1582102330688" style="zoom:100%;" title="1582102330688" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582102330688.png" class="lazyload"></a></p>
<h2 id="2-符号表示"><a href="#2-符号表示" class="headerlink" title="2. 符号表示"></a>2. 符号表示</h2><p>训练和测视频序列： ${I = \{I_i ∈ R^{w<em>h</em>3} \}_{i = 1}^N}$     大小是 473 x 473</p>
<p>ZVOS的目的是：产生相关帧的二值分割掩码:    ${S= \{ S_i ∈ \{0, 1\}^{W*H}\}_{i = 1}^N }$</p>
<p>AGNN将$I $ 表示成有向图 $G = (V, E)$:</p>
<ul>
<li>节点<code>node</code>  $v_i ∈ V$， 代表第 $i$ 帧 $I_i$  .</li>
<li>边 <code>edge</code>  $e_{ij} = (v_i,  v_j) ∈ E$ ,  代表 $I_i 到 I_j 的关系$。</li>
</ul>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582104154693.png" data-fancybox="group" data-caption="1582104154693" class="fancybox"><img alt="1582104154693" style="zoom:80%;" title="1582104154693" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582104154693.png" class="lazyload"></a></p>
<p>​        AGNN的核心思想是在G上执行K个消息传播迭代，以有效地挖掘  $ I $  各个节点（帧）间丰富的高阶关系。这有助于更好地从全局视图捕获视频内容，并获得更准确的前景估计。内注意通过关注同一节点嵌入内的所有位置来计算某个位置的响应。</p>
<p>​        然后从最后的节点状态 $ \{h_i^K\}_{i = 1}^N$  使用读出函数得到 分割的预测 $ \hat{S}$ </p>
<h2 id="FCN-Based-Node-Embedding"><a href="#FCN-Based-Node-Embedding" class="headerlink" title="FCN-Based Node Embedding"></a>FCN-Based Node Embedding</h2><p>我们利用DeepLabV3—-一个经典的基于FCN的语义分割架构，提取有效的帧特征，作为节点表示。</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582105002179.png" data-fancybox="group" data-caption="1582105002179" class="fancybox"><img alt="1582105002179" title="1582105002179" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582105002179.png" class="lazyload"></a></p>
<p>对于一个节点$v_i$  , 初始的 embedding $h_i^0$ 计算：</p>
<script type="math/tex; mode=display">
h_i^0 =  V_i =  F_{DeepLab}(I_i) ∈ R^{W * H * C}</script><p>  $h_i^0$是一个三维的Tensor， 保存了一些空间信息和语义信息。</p>
<h2 id="Intra-Attention-Based-Loop-Edge-Embedding"><a href="#Intra-Attention-Based-Loop-Edge-Embedding" class="headerlink" title="Intra-Attention Based Loop-Edge Embedding"></a>Intra-Attention Based Loop-Edge Embedding</h2><p>Loop Edge :  $e_{i,i} ∈ E $ ，  Loop Edge Embeddings :  $e_{i,i}^k$  用于捕捉帧内的节点表示（$ h_i^k$）之间的关系。</p>
<p>把 $e_{i,i}^k$  当做一种   <code>intra-attention mechanism</code>  ：比如（  Non-local neural networks  、Attention is all有need）。 有助于建模的长期，多层次的依赖图像区域 ，即可以捕获图像的长期依赖，属于self-attention（  Self-attention generative adversarial networks. In ICML, 2019. ）。 <code>Intra-Attention</code> 通过关注同一节点Embeddings内的所有位置来计算某个位置的响应 。</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582105775054.png" data-fancybox="group" data-caption="1582105775054" class="fancybox"><img alt="1582105775054" style="zoom:100%;" title="1582105775054" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582105775054.png" class="lazyload"></a></p>
<blockquote>
<p>*代表卷积操作</p>
<p>W 代表可以训练的卷积核</p>
<p> α 是可学习的尺度参数 </p>
</blockquote>
<p>上面公式使得 $h_i^k$ 中每个位置的输出元素在对上下文信息进行编码的同时，也对其原始信息进行编码，从而提高了表达能力。</p>
<h2 id="Inter-Attention-Based-Line-Edge-Embedding"><a href="#Inter-Attention-Based-Line-Edge-Embedding" class="headerlink" title="Inter-Attention Based Line-Edge Embedding"></a>Inter-Attention Based Line-Edge Embedding</h2><p>line edge :  $e_{ij} ∈ E$  连接两个不同的节点。 line-edge Embedding： $e_{i,j}^k$ 用于挖掘两个节点之间的关系。</p>
<blockquote>
<p><code>inter-attention mechanism</code>  ：  <a href="https://arxiv.org/pdf/1606.00061.pdf" target="_blank" rel="noopener">《Hierarchical question-image co-attention for visual question     answering. In NIPS, 2016.  》</a></p>
</blockquote>
<p>使用inter-attention mechanism来捕获两个节点之间的双向关系：</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582111196501.png" data-fancybox="group" data-caption="1582111196501" class="fancybox"><img alt="1582111196501" style="zoom:100%;" title="1582111196501" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582111196501.png" class="lazyload"></a></p>
<p>$ e_{i,j}^k$ =  $e_{j,i}^k$   。 对于节点$v_i$来说，  $ e_{i,j}^k$  表示 输出边的特征，  $e_{j,i}^k$表示输入边的特征。 $W_c  ∈ ^{C * C}$是一个可学习的权重矩阵。</p>
<p>$h_i^k ∈ R^{(WH) <em> C}$和$h_j^k ∈ R^{(WH) </em> C}$ 被展平成矩阵的形式。 $ e_{i,j}^k$的每个元素 反映了 $h_i^k$的每一行与$h_j^k$的每一列之间的相似度。</p>
<p>所以，$ e_{i,j}^k$ 就可以看做节点$v_i$的Embedding 对节点 $v_j$的重要性， 反之亦然。</p>
<h2 id="Gated-Message-Aggregation"><a href="#Gated-Message-Aggregation" class="headerlink" title="Gated Message Aggregation"></a>Gated Message Aggregation</h2><p>在AGNN中，对于在loop-edge中传递的message，将环边嵌入向量  $e_{i,j}^{k-1}$  本身视为一个message，因为它已经包含了上下文和原始节点信息 。</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582112367589.png" data-fancybox="group" data-caption="1582112367589" class="fancybox"><img alt="1582112367589" style="zoom:100%;" title="1582112367589" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582112367589.png" class="lazyload"></a></p>
<p>$m_{j,i}^k$   : 表示 $v_j$  传递到 $v_i$ 的message， 从而有：</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582112571079.png" data-fancybox="group" data-caption="1582112571079" class="fancybox"><img alt="1582112571079" style="zoom:100%;" title="1582112571079" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582112571079.png" class="lazyload"></a></p>
<p>  softmax(·)  normalizes each row of the input  .</p>
<p>因此： $m_{j,i}^k$  的每一行是 $h_i^{k-1}$ 的每一行(位置)的加权组合，其中权值来自 $e_{i, j}^{k-1}$  的对应列。  通过这种方式，消息函数M(·)分配其边缘加权特征(即，消息)到邻居节点 。(  <a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">Graph attention networks</a>. In ICLR, 2018. )</p>
<p>然后$m_{j,i}^k$ 被重新reshape成 一个三维张量 W <em> H </em> C。</p>
<p>此外，由于某些节点由于摄像机移位或视野外而产生噪声，因此它们的message可能是无用的甚至有害的。我们应用一个可学习的 门G(·) 来评估一个消息 $m_{j,i}^k$ 的置信度 。</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582113546163.png" data-fancybox="group" data-caption="1582113546163" class="fancybox"><img alt="1582113546163" style="zoom:100%;" title="1582113546163" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582113546163.png" class="lazyload"></a></p>
<p>$F_{GAP}(.)$  表示使用全局平均池化来对通道之间作出响应。 $\sigma$  表示 sigmoid函数。$W_g$ 和 $b_g$ 表示卷积核参数和偏置。</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582113956308.png" data-fancybox="group" data-caption="1582113956308" class="fancybox"><img alt="1582113956308" style="zoom:100%;" title="1582113956308" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582113956308.png" class="lazyload"></a></p>
<p> 这里，门机制用于过滤噪声帧中不相关的信息。 </p>
<h2 id="ConvGRU-based-Node-State-Update-更新节点状态"><a href="#ConvGRU-based-Node-State-Update-更新节点状态" class="headerlink" title="ConvGRU based Node-State Update[更新节点状态]"></a>ConvGRU based Node-State Update[更新节点状态]</h2><p>在第k次迭代，在收集到所有相邻节点和它自身的信息（$m_i$）后 ，通过将先前的状态$h_i^{k-1}$和 它接收到的消息$m_i^k$       要一起考虑，$v_i$变成一个新的状态$h_i^k$,  为了保留$h_i^{k-1}$和$m_i^k$   的时间信息，使用ConVGRU来更新节点的状态：</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582114662750.png" data-fancybox="group" data-caption="1582114662750" class="fancybox"><img alt="1582114662750" style="zoom:80%;" title="1582114662750" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582114662750.png" class="lazyload"></a></p>
<h2 id="Readout-Function-读出函数-预测"><a href="#Readout-Function-读出函数-预测" class="headerlink" title="Readout Function  [读出函数-预测]"></a>Readout Function  [读出函数-预测]</h2><p>在进行了K次迭代的消息传递之后，获得了每个节点$v_i$的最终状态  $ h_i^K $ ,  在读出的阶段通过 <strong>读出函数R()</strong>  获得了分割的预测图 $\hat{S} ∈ [0, 1]^{W * H}$ 。将最终状态节点$ h_i^K $  与 原始节点 $V_i$   concatenate 之后使用读出函数的到预测结果：</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582114987509.png" data-fancybox="group" data-caption="1582114987509" class="fancybox"><img alt="1582114987509" style="zoom:100%;" title="1582114987509" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582114987509.png" class="lazyload"></a></p>
<p>再次，为了保存空间信息，将读出函数实现为一个小型的FCN网络，该网络由三个卷积层和一个<code>sigmoid</code>函数将预测归一化为[0， 1]。</p>
<p>​        在intra-attention和update function 中，卷积运算是通过1×1个卷积层来实现的。读出函数由两个3×3个卷积层组成，每个卷积层有一个1×1个卷积层。作为基于GNN模型的消息传递，这些函数在所有节点之间共享权重。此外，以上所有函数都经过精心设计，避免了干扰空间信息，这对于ZVOS是必不可少的，因为它是一个像素级的预测任务 。</p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582114107708.png" data-fancybox="group" data-caption="1582114107708" class="fancybox"><img alt="1582114107708" style="zoom:100%;" title="1582114107708" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582114107708.png" class="lazyload"></a></p>
<hr>
<h1 id="六、-网络的具体信息"><a href="#六、-网络的具体信息" class="headerlink" title="六、 网络的具体信息"></a>六、 网络的具体信息</h1><p>整个模型是端到端的。</p>
<p>特征提取： DeepLadV3的前5个卷积块 对每个节点 得到初始的状态 $V_i$ =  $h_i^0$  ∈ $R^{60 <em> 60 </em> 256}$</p>
<p>然后经过K次迭之后得到分割预测图$\hat{S} ∈ [0 ,1] ^{60 × 60}$。</p>
<h2 id="训练：损失函数"><a href="#训练：损失函数" class="headerlink" title="训练：损失函数"></a>训练：损失函数</h2><p>  binary cross entropy loss   : </p>
<p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582115711854.png" data-fancybox="group" data-caption="1582115711854" class="fancybox"><img alt="1582115711854" style="zoom:100%;" title="1582115711854" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582115711854.png" class="lazyload"></a></p>
<p>值得一提的是，由于AGNN在同一时间处理多个视频帧，因此在组合候选帧数量众多的情况下，它带来了一种非常有效的训练数据扩充策略。在我们的实验中，在训练过程中，由于计算的限制，我们从训练视频集中随机选择2个视频，每个视频采样3帧(N0 = 3)。另外，我们将迭代总数设为K = 3。</p>
<h1 id="七、-结果"><a href="#七、-结果" class="headerlink" title="七、 结果"></a>七、 结果</h1><p><a href="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582115863806.png" data-fancybox="group" data-caption="1582115863806" class="fancybox"><img alt="1582115863806" style="zoom:100%;" title="1582115863806" data-src="/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/1582115863806.png" class="lazyload"></a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">朱宏梁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://castile.github.io/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/">https://castile.github.io/2020/02/19/Zero-Shot%20Video%20Object%20Segmentation%20via%20Attentive%20Graph%20Neural%20Networks/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://Castile.github.io">Castile</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/papers/">papers    </a><a class="post-meta__tags" href="/tags/VOS/">VOS    </a></div><div class="post_share"><div class="social-share" data-image="/img/cover/agnn.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/02/17/Java%E4%B9%8BIO%E6%B5%81/"><img class="next_cover lazyload" data-src="/img/cover/io.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Java之IO流</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/02/05/CosNet-基于协同注意孪生网络的无监督视频目标分割/" title="CosNet:基于协同注意孪生网络的无监督视频目标分割"><img class="relatedPosts_cover lazyload"data-src="/img/cover/cosnet.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-05</div><div class="relatedPosts_title">CosNet:基于协同注意孪生网络的无监督视频目标分割</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '1a98a067225d26dccaeb',
  clientSecret: 'ded51ee8abb6454d29fa5eeb07b88441246e4971',
  repo: 'Castile.github.io',
  owner: 'Castile',
  admin: 'castile',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
}</script></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 朱宏梁</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/av-min.js"></script><script src="/js/valine.min.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script src="https://cdn.jsdelivr.net/npm/activate-power-mode/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true; 
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/ClickShowText.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>